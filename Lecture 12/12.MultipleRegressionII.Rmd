---
title: 'Lecture 12: Multiple Regression part II: Multicollinearity'
author: "Zachary Marion"
date: "3/7/2018"
output: pdf_document
fontsize: 12pt 
geometry: margin=0.75in
---

```{r setup, include=FALSE}
library(knitr)
library(extrafont)
loadfonts()
setwd("~/Dropbox/BayesClass/2018 Class/Lecture 12")
knitr::opts_chunk$set(
  echo = TRUE,
  comment = NA,
  fig.align = "center",
  fig.height = 3,
  fig.width = 4
  )
```
\emph{* This lecture is based on chapter 5 of Statistical Rethinking by Richard McElreath.}

As always, we need to load some packages and set some options prior to running any models:

```{r stanPackages, echo=TRUE, message=FALSE, warning=FALSE, results='hide'}
library(rstan)
library(shinystan)
library(car)
library(xtable)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
source("../utilityFunctions.R")
```

In the previous lecture, we discussed why we might want multiple predictors in the same regression model and learned how to set up these models in `Stan`. 

We also learned about how to visualize multiple regressions using things like counterfactual plots. 

As a recap, the model was specified as: 
\begin{align}
  obs_i    &\sim \mathrm{Normal}(\mu_i, \sigma)  \nonumber  \\ 
  \mu_i     &= \alpha + \sum_{j=1}^n \beta_j x_{ji}   \nonumber  \\
            &= \alpha + \beta_1 x_{1i} + \beta_2 x_{2i} + \ldots + \beta_n x_{ni} \\
  \alpha  &\sim \mathrm{Normal}(a_\mu, a_{SD})      \nonumber   \\
  \beta_j   &\sim \mathrm{Normal}(b_\mu, b_{SD})        \nonumber   \\
  \sigma  &\sim \mathrm{Cauchy^+}(0, \sigma_{SD})      \nonumber
\end{align}

If multiple predictors are the bee's knees and improve model inference, why not add all possible predictors? There are at least three good reasons:

1. *Multicollinearity:* a strong correlation between two or more predictor variables.

2. *Post-treatment bias:* mistaken inferences arising from variables that are consequences of other variables 
   
    * opposite of *omitted variable bias* which we discussed last time. 

3. *Overfitting:* We will talk about this in a few weeks.

\section{Multicollinearity}

The consequence of multicollinearity (as we will see) is that the posterior distribution will suggest a huge range of plausible parameter values, from small associations to enormous ones.

This occurs even if all variables truly have strong associations with the response variable.

\subsection*{Simulation of multicollinearity}

This example is rather artificial, but may make sense to people with an anthropology background.

*Example:* Prediction of an individual's height using leg length as predictors. We would expect that height is positively associated with leg length, but strange(r) things happen if we put both leg lengths into the model!

We will simulate heights and leg lengths for $N=100$ individuals in 3 steps:
    
1. First we simulate heights from a Gaussian distribution
```{r, eval=FALSE}
N <- 100                  # number of individuals
height <- rnorm(N, 10, 2) # simulated heights 
```  
   
2. Then we simulate leg lengths as proportions (0.4--0.5) of height
    
```{r, eval=FALSE}
legProp <- runif(N, 0.4, 0.5) # leg as proportion of height
```

3. Each leg gets measurement/developmental error tacked on so the right and left legs are not exactly equal in length, as is typical
```{r, eval=FALSE}
leftLeg <- height*legProp + rnorm(N, 0, 0.02) # sim left leg
rightLeg <- height*legProp + rnorm(N, 0, 0.02)# sim right leg
```

```{r, echo=FALSE}
set.seed(2)
N <- 100                  # number of individuals
height <- rnorm(N, 10, 2) # simulated heights

legProp <- runif(N, 0.4, 0.5) # leg as proportion of height
leftLeg <- height*legProp + rnorm(N, 0, 0.02) # sim left leg
rightLeg <- height*legProp + rnorm(N, 0, 0.02)# sim right leg
```

Before fitting the model, what should we expect? 

  * On average, leg length should be 45% of an individual's height for these data. 
  
  * $\beta_{leg}$ measures the relationship with leg length and height.
  
  * Therefore, $\beta_{leg}$ should equal the average height (10) divided by 45% of the average height (4.5) $\Rightarrow 10/4.5 \approx 2.2$.
  
Using a multiple regression model similar to that from last time (`12multMod.stan`):
```{r, eval=FALSE}
data {
  int<lower=0> nObs;
  int<lower=0> nVar;      // no. vars
  vector[nObs] obs;
  matrix[nObs, nVar] X;  
  real<lower=0> aMu;      // mean of prior alpha
  real<lower=0> aSD;      // SD of prior alpha
  real<lower=0> bMu;      // mean of prior betas
  real<lower=0> bSD;      // SD of prior beta
  real<lower=0> sigmaSD;  // scale for sigma
}

parameters {
  real alpha;
  vector[nVar] beta;
  real<lower=0> sigma;
}

transformed parameters {
  // can be useful for plotting purposes
  vector[nObs] mu;
  mu = alpha + X*beta;
}

model {
  alpha ~ normal(aMu, aSD);
  beta ~  normal(bMu, bSD);
  sigma ~ cauchy(0, sigmaSD);

  obs ~ normal(mu, sigma);
}

```

```{r engine = 'cat', engine.opts = list(file = "12.multMod.stan", lang = "stan"), echo=FALSE}
data {
  int<lower=0> nObs;
  int<lower=0> nVar;      // no. vars
  vector[nObs] obs;
  matrix[nObs, nVar] X;
  real<lower=0> aMu;      // mean of prior alpha
  real<lower=0> aSD;      // SD of prior alpha
  real<lower=0> bMu;      // mean of prior betas
  real<lower=0> bSD;      // SD of prior beta
  real<lower=0> sigmaSD;  // scale for sigma
}

parameters {
  real alpha;
  vector[nVar] beta;
  real<lower=0> sigma;
}

transformed parameters {
  // can be useful for plotting purposes
  vector[nObs] mu;
  mu = alpha + X*beta;
}

model {
  alpha ~ normal(aMu, aSD);
  beta ~  normal(bMu, bSD);
  sigma ~ cauchy(0, sigmaSD);

  obs ~ normal(mu, sigma);
}

```

let's see what happens:

```{r, message=FALSE, warning=FALSE, cache=TRUE, verbose=FALSE}
dat <- list(nObs=N, nVar=2, obs=height, X=cbind(leftLeg, rightLeg),
  aMu=10, aSD=10, bMu=2, bSD=10, sigmaSD=10)

legsMod <- stan(file="12.multMod.stan", data=dat, iter=2000,
 chains=4, seed=867.5309, pars="mu", include=FALSE)
```

```{r, fig.height=2, fig.width=5, message=FALSE, warning=FALSE}
plotLegs <- plot(legsMod, pars=c("alpha", "beta", "sigma"), ci_level=0.5)
plotLegs + theme(text=element_text(family="ArialMT"))
```

```{r}
round(summary(legsMod, pars=c("alpha", "beta", "sigma"),  
  probs = c(0.025, 0.5, 0.975))$summary,2)
```

Both the plot and printed output show that something has gone wrong. The means are weird and the uncertainties are huge! 

If both legs have almost identical lengths and height is so strongly correlated with leg length, why is this posterior distribution so odd? 
  
The model did exactly what it should have, and the posterior distribution is exactly what we asked for. 

  * A multiple regression answers: *What is the value of knowing each predictor after knowing all of the other predictors?*
    
    * i.e., *What can we say about height knowing the right (left) leg length after already knowing the left (right) leg length?* 

It will help to look at the bivariate posterior distribution of $\beta_{Left}$ and $\beta_{Right}$:  

```{r, fig.height=3, fig.width=4, echo=c(-1)}
par(mar=c(3,3.2,0.1,0.5))
col <- "#50505010"

legs <- as.matrix(legsMod, pars="beta")
# Plot alpha and beta
plot(legs, pch=16, col=col, las=1, xlim=c(-3,12), 
  ylim=c(-10,5), bty="l", ann=FALSE)
dataEllipse(legs,level=c(0.95), add=TRUE, labels=FALSE,
  plot.points=FALSE, center.pch=FALSE, col=c(col,"#006DCC"))

mtext(text=expression(beta[R]), side=2, line=2.1, cex=1.2, las=1)
mtext(text=expression(beta[L]), side=1, line=2, cex=1.2)
```

The posterior distribution of $\beta_L$ and $beta_R$ is highly correlated ($\rho$ = `r round(cor(legs[,1],legs[,2]),4)`). 

  * when $\beta_L$ is large, $\beta_R$ must be small. 
  
  * Both leg variables contain almost the same information.
     
      * including both of them in the model results in an almost infinite number of combinations of $\beta_L$ & $\beta_R$ that produce the same predictions. 
      
We can think of this as approximating the following likelihood:
\begin{align}
  y_i    &\sim \mathrm{Normal}(\mu_i, \sigma)  \nonumber  \\ 
  \mu_i     &= \alpha + \beta_1 x_{i} + \beta_2 x_{i}   \nonumber  
\end{align}
where $y$ is the outcome (e.g., `height`) and x is a single predictor (e.g., `leg lengths`) used twice. 

From the computer's point of view, this is: 
\begin{equation}
  \mu_i = \alpha + (\beta_1 + \beta_2)x_{i}.   \nonumber 
\end{equation}
We cannot separate out $\beta_L$ and $\beta_R$ because they do not separately influence the mean $\mu$. 

  * Only their sum $\beta_L + \beta_R$ does. 
 
  * This means there are infinite combinations of $\beta_L$ & $\beta_R$ that make the sum close to the actual association of `leg length` with `height`.
  
And our model has done exactly that. If we compute the posterior distribution of the $\beta's$ sum and plot it:
        
```{r, fig.height=2.5, fig.width=3}
par(mar=c(3,3.2,0.1,0.5))
plotInterval(rowSums(legs), HDI=TRUE, interval=0.95, xlims=c(1.8, 2.4),
  col="cornflowerblue", yOffset=0.1)
mtext(expression(paste(beta[L] + beta[R])), side=1, line=2.1, cex=1.2)
```
The posterior mean is a bit over 2 and the SD (`r round(sd(rowSums(legs)),2)`) is much less than the SD for either $\beta$ ($\approx 2.4$).

If we fit a regression with just one of the variables (`12.uniMod.stan`), we get about the same posterior mean:

```{r, message=FALSE, warning=FALSE, cache=TRUE, verbose=FALSE}
dat1 <- list(nObs=N, nVar=2, obs=height, xvar=leftLeg, 
  aMu=10, aSD=10, bMu=2, bSD=10, sigmaSD=10)

legMod <- stan(file="12.uniMod.stan", data=dat1, iter=2000, chains=4, 
  seed=2, pars=c("alpha","beta", "sigma"))

round(summary(legMod, probs = c(0.025, 0.5, 0.975))$summary,2)
```

\paragraph{So how large does a correlation have to become before we should start worrying?}

This question will depend on the situation and priors, but what matters is not just the correlation between pairs of variables.

It is the correlation that remains after accounting for other predictors.
  
However, with only two predictor variables, we can do a little simulation experiment to address the correlation question directly.

1. Suppose we only have height and the left leg length.

2. We can make a random predictor ($x$) that is correlated with $leg_{L}$ at a predetermined $\rho$. 

3. We will fit a regression model that predicts height using $leg_{L}$ and $x$.

4. Record the SE of the estimated effect of $leg_{L}$

5. Repeat multiple iterations for many values of $\rho$.

The following code will do exactly that:
```{r, cache=TRUE}
# function to simulate collinearity for 1 iter
collSim <- function (rho=0.9) {
  x <- rnorm(N, mean=rho*leftLeg, sd=(1-rho^2)*var(leftLeg))
  m <- lm(height ~ leftLeg + x)
  return(sqrt(diag(vcov(m))[2]))
}

# function to replicate collSim iter times for a given rho
repColSim <- function (rho=0.9, iter=100) {
  stdev <- replicate(iter, collSim(rho))
  return(mean(stdev))
}

rhoSeq <- seq(0, 0.99, by=0.01)  # sequence of correlations
stddev <- sapply(rhoSeq, function(z) {repColSim(rho=z, iter=100)})
```  

```{r, fig.height=2.5, fig.width=3, echo=-1}
par(mar=c(3,3.6,0.1,0.5))
plot(rhoSeq, stddev, type="l", bty="l", ann=FALSE, las=1, col="red", lwd=2)
mtext(text=expression(bar(SD[beta])), side=2, line=2.1, cex=1.2)
mtext(text = expression(rho), side=1, line=2, cex=1.2)
```

Here the $y$ axis is the average SD across 100 replications per $\rho$ using a simulated correlated predictor variable $x$.

  * When the two variables are uncorrelated, the SD of the posterior is small. As the correlation gets larger, the SD inflates. 
  
  * The inflation is also non-linear. When $\rho > 0.9$, it accelerates rapidly towards $\infty$. 
  
    * Note that this model is essentially Bayesian and assuming flat uninformative priors. More informative priors can help mitigate some of the effects of multicollinearity.
 
Multicollinearity is a type of *non-identifiability*. This means the structure of the data and/or model make it impossible to estimate the values of parameters. 

In our example, this is because there are an infinite number of combinations of parameters that are equally plausible. 

\section*{Post-treatment bias}
Another pitfall of multiple regression arises from mistaken inferences due to including variables that are consequences of other variables. 

As a motivating ecological example, suppose we have a greenhouse experiment with plants (maybe \emph{Eucalyptus}?) and want to understand the impact of fungal treatments on growth. 
  
  * Plants are germinated and let grow for a set amount of time, and then their heights $H_0$ are measured. 
  
  * We then apply a fungal treatment to half of the replicates' soils.
  
  * Final heights $H_1$ and the presence of soil fungus are then measured.
  
We have four variables of interest: initial and final heights, treatment, and the presence of fungus. 

If causal inference about the effect of treatment is of interest however, we shouldn't include fungal presence because it is a post-treatment effect.

I have simulated such an example and the results are saved as `plants.csv`. The simulation code is below:

```{r, eval=FALSE}
# Number of plants
N <- 100
set.seed(3)
# initial heights
h0 <- rnorm(N, 10, 2)

# assign treatments and simulate growth and fungal persistence
treatment <- rep(0:1, each=N/2)
fungus <- rbinom(N, size=1, prob=0.5-treatment*0.4)
h1 <- h0 + rnorm(N, 5-3*fungus)

plants <- data.frame(h0=h0, h1=h1, treats=treatment, fungus=fungus)
write.csv(plants, "plants.csv", row.names=FALSE)
```

```{r}
plants <- read.csv("plants.csv")
head(plants)

obs <- plants$h1
# create a design matrix with initial height centered but not scaled
X <- cbind(as.vector(scale(plants$h0, scale=FALSE)), 
  plants$treats, plants$fungus)
```

First off, this is our first example with *dummy coding*. Dummy variables encode qualitative categories (e.g., treatment) into quantitative models. 

  * The effect of dummy coding is to turn variables on (1) or off (0) in each category. 

For example, if we just created a model of the effect of treatment on final height:

\begin{align}
  h_{1i}    &\sim \mathrm{Normal}(\mu_i, \sigma)  \nonumber  \\ 
  \mu_i     &= \alpha + \beta_{treat} x_{i}    \nonumber  
\end{align}

the parameter $\beta_{treat}$ only influences prediction for cases where `treats` = 1. When `treats` = 0, it has no effect because the model is $\alpha + \beta_{treats}\times 0$. 

  * The intercept is the mean value when `treats` = 0.

In the full model, $\alpha$ is the average value when `treats` = 0, `fungus` = 0, and the initial height (`h0`) is at it's average (because we centered for interpretability).

```{r, message=FALSE, warning=FALSE, cache=TRUE, verbose=FALSE}
dat <- list(nObs=nrow(plants), nVar=dim(X)[2], obs=obs, X=X, 
  aMu=10, aSD=10, bMu=0, bSD=10, sigmaSD=10)

plantMod <- stan(file="12.multMod.stan", data=dat, iter=2000, 
  chains=4, seed=2, pars=c("alpha","beta", "sigma"))

round(summary(plantMod, pars=c("alpha", "beta", "sigma"), 
  probs = c(0.025, 0.5, 0.975))$summary,2)
```

If we look at the marginal posterior for $\beta_{treats}$ (`beta[2]`), the effect of treatment is actually negative but for all intents and purposes has a negligible effect.

The initial height (`beta[1]`) and fungal presence (`beta[3]`) have important effects. Does treatment not matter?

The problem is that fungal presence is predominantly a consequence of treatment. When we include `fungus` in the model, we are asking *Once we already know whether a plant has developed a fungal infection, does soil treatment matter?* 

  * The answer is no because the soil treatment affects plant growth by preventing fungal growth. If the fungus grew, qualitatively it doesn't matter whether the plants were treated or not.
  
  * Given  the experimental design, we want to know the impact of treatment on growth.
  
  * If we exclude fungus:
  
```{r, message=FALSE, warning=FALSE, cache=TRUE, verbose=FALSE}
X1 <- cbind(as.vector(scale(plants$h0, scale=FALSE)), 
  plants$treats)

datNF <- list(nObs=nrow(plants), nVar=dim(X1)[2], obs=obs, X=X1, 
  aMu=10, aSD=10, bMu=0, bSD=10, sigmaSD=10)

modNF <- stan(file="12.multMod.stan", data=datNF, iter=2000, 
  chains=4, seed=2, pars=c("alpha","beta", "sigma"))

round(summary(modNF, pars=c("alpha", "beta", "sigma"), 
  probs = c(0.025, 0.5, 0.975))$summary,2)
```

Now the impact of fungal treatment `beta[2]` = 1.17 is substantial and positive. 

Including pre-treatment variables as controls (e.g., initial plant height) makes sense because otherwise they might mask causal effects of treatment. 

In rigorously controlled experimental studies it is (hopefully) easy to identify pre- vs. post-treatment variables. 

  * But post-treatment variables can mask the effects of the treatment itself.

The problem with post-treatment variables is that sometimes (e.g., observational studies), it is hard to identify them.

  * Moreover, model selection (which we will cover soon) may not help, because a model that includes both `fungus` and `treatment` fits the data better and makes better predictive inferences. 
 
   * The full model would mislead because it is asking the wrong question, not because it is making poor predictions.
   
\section*{Categorical variables}
Thus far, we've had a brief think about dummy coding as on-off switches for  categorical variables. Now we are going to expand that thought process to encompass many categories. 

Binary categories are easy to think about because we only need one dummy variable. The other category is the intercept $\alpha$. 

When there are more than 2 categories, we need $k-1$ dummy variables, where $k$ = the number of groups. 

  * Each $k-1$ dummy variable represents, with  value 1, a unique category. The $kth$ category ends up again as the intercept.

Let's go back to the milk dataset again (full data; `milkFull.csv`), where we were interested in the energy content (`kcal`$\cdot g^{-1}$) of primates.   

  * Now we are interested in the `clade` variable, which encompasses broad taxonomic categories of the primate species.   

```{r}
milk <- read.csv("milkFull.csv")
unique(milk$clade)
```
  
There are four categories and none of them has yet been coded as dummy variables. 
  
To make dummy variables for the `Streppsirrhine` category:
```{r}
(strep <- ifelse(milk$clade == "Strepsirrhine", 1, 0))
```

only those rows (observations) where `clade == "Strepsirrhine` get a 1. 

We can make the rest of the dummy variables with the same strategy and `cbind` them together to make our design matrix:
```{r}
nwm <- ifelse(milk$clade == "New World Monkey", 1, 0)
owm <- ifelse(milk$clade == "Old World Monkey", 1, 0)
clade <- cbind(nwm, owm, strep)
```

Note that we don't need for an `Ape` dummy variable because it will be the default intercept category.
  
  * Including a dummy variable for `Ape` as well will result in a non-identified model. *Why?*

Another (easy) way to create a design matrix is to use the `model.matrix` function in `R`. 

```{r, eval=FALSE}
X <- model.matrix(~clade, data=milk)
X <- as.matrix(X) # needed to get rid of the attribute list
```

By default `model.matrix` orders categories alphabetically, and the first category is a vector of 1's for the intercept. 

  * If we want to specify the intercept separately (as $\alpha$) then drop the first column. Otherwise we could just have a $k$-length vector of $\beta's$. 
  
Our model is then specified as   
\begin{align}
  obs_i    &\sim \mathrm{Normal}(\mu_i, \sigma)  \nonumber  \\ 
  \mu_i     &= \alpha + \sum_{j=2}^k \beta_k x_{ji}   \nonumber  \\
            &= \alpha + \beta_1 x_{1i} + \beta_2 x_{2i} + \ldots + \beta_{k} x_{ki} \\
  \alpha  &\sim \mathrm{Normal}(a_\mu, a_{SD})      \nonumber   \\
  \beta_j   &\sim \mathrm{Normal}(b_\mu, b_{SD})        \nonumber   \\
  \sigma  &\sim \mathrm{Cauchy^+}(0, \sigma_{SD}).      \nonumber
\end{align}

Here is how dummy variables assign individual parameters based on category:


Category       NWM$_i$      OWM$_i$    Strep$_i$   $\mu_i$
---------     ---------    ---------  -----------  ----------
Ape             0           0           0          $\mu_i=\alpha$
NW Monkey       1           0           0          $\mu_i = \alpha + \beta_1$
OW Monkey       0           1           0          $\mu_i = \alpha + \beta_2$
Strepsirrhine   0           0           1          $\mu_i = \alpha + \beta_3$
----------------------------------------------------------------------------

Fitting the model (`12.multMod.stan`) is identical to last time. 

```{r, message=FALSE, warning=FALSE, cache=TRUE, verbose=FALSE}
nObs <- nrow(milk)
nVar <- ncol(clade)
obs <- milk$kcal
X   <- clade
aMu <- bMu <- 0.6         
aSD <- sigmaSD <- 10
bSD <- 1

dat <- list(nObs=nObs, nVar=nVar, obs=obs, X=X, aMu=aMu, aSD=aSD, 
  bMu=bMu, bSD=bSD, sigmaSD=sigmaSD)

milkMod <- stan(file="12.multMod.stan", data=dat, iter=2000,
 chains=4, seed=867.5309, pars="mu", include=FALSE)

round(summary(milkMod, pars=c("alpha", "beta", "sigma"),  
  probs = c(0.025, 0.5, 0.975))$summary,2)
```

The marginal posterior estimate $\alpha$ is the average milk energy for apes.

  * All other categories are deviations from `Ape`. 
 
  * To get their posterior distributions of average milk energy for each category:

```{r, echo=-7, results="as.is"}
# Extract Posterior estimates
post <- as.matrix(milkMod, pars=c("alpha","beta"))

# bind together the intercept, and then for beta 2-4 use 
# Sweep function to add them to the intercept
mu <- cbind(post[,1], sweep(post[,2:4], MARGIN=1, STATS = post[,1], FUN='+'))

# Calculate column means for mu
means <- colMeans(mu)

# Calculate  column SD
SD <- apply(mu, 2, sd)

# Calculate 95% HDI
muHDI <- apply(mu, 2, HDI, credMass=0.95)

# put everything together
sumTab <- data.frame(Mean=means, SD=SD, lower.95=muHDI[1,],
  upper.95=muHDI[2,], row.names=c("Ape", "NWM", "OWM", "Strep"))
```  

```{r xtable, results='asis', echo=FALSE}
print(xtable(sumTab, digits=2, align = c("l",rep("c",4))))
```

```{r, fig.height=2.6, fig.width=7}
par(mar=c(3,3.2,0.1,0.5))
par(mfrow=c(1,2))
plot(density(mu[,2], adj=2), las=1, col="blue", lwd=2, main="", 
  xlim=c(0.5, 1))
lines(density(mu[,3], adj=2), col="black", lwd=2)
mtext(text = "Marginal milk energy", side=1, line = 2, cex=1)
text(0.6,8, "New World\nMonkeys", col="blue")
text(0.9,7, "Old World\nMonkeys", col="black")

# Contrast plot
dif <- mu[,3]-mu[,2]
plotInterval(dif, HDI = TRUE, interval = 0.95, xlims=c(-0.2,0.3), col="#50505080")
mtext(text = "Joint milk energy difference", side=1, line = 2, cex=1)
```
Going back to the `New World`  vs. `Old World` comparison, plotting the marginal densities suggests substantial overlap between the two categories. 


However, plotting the joint distribution of the difference suggests that the two types of monkeys differ substantially in their milk energies (HDI: -0.06, 0.22). The proportion of differences below zero is only `r round(sum(mu[,3]<mu[,2])/nrow(mu),2)`; much less than the marginal distributions would suggest. 

\subsection*{Unique intercepts}

Another way to model categorical variables is to construct a vector of intercept parameters, one for each category and then use index variables. 
  
  * This is very similar to what we did earlier when we made hierarchical models.
```{r eval=FALSE}
data {
  int<lower=0> nObs;
  int<lower=0> nVar;      // no. vars
  vector[nObs] obs;
  int x[nObs];
  real<lower=0> aMu;      // mean of prior alpha
  real<lower=0> aSD;      // SD of prior alpha
  real<lower=0> sigmaSD;  // scale for sigma
}

parameters {
  vector[nVar] alpha;
  real<lower=0> sigma;
}

model {
  alpha ~ normal(aMu, aSD);
  sigma ~ cauchy(0, sigmaSD);
  {
    vector[nObs] mu;
    mu = alpha[x];
    
    obs ~ normal(mu, sigma);  
  }
}

```  
```{r engine = 'cat', engine.opts = list(file = "12.interceptMod.stan", lang = "stan"), echo=FALSE}
data {
  int<lower=0> nObs;
  int<lower=0> nVar;      // no. vars
  vector[nObs] obs;
  int x[nObs];
  real<lower=0> aMu;      // mean of prior alpha
  real<lower=0> aSD;      // SD of prior alpha
  real<lower=0> sigmaSD;  // scale for sigma
}

parameters {
  vector[nVar] alpha;
  real<lower=0> sigma;
}

model {
  alpha ~ normal(aMu, aSD);
  sigma ~ cauchy(0, sigmaSD);
  {
    vector[nObs] mu;
    mu = alpha[x];
    
    obs ~ normal(mu, sigma);  
  }
}

```

```{r, message=FALSE, warning=FALSE, cache=TRUE, verbose=FALSE}
obs <- milk$kcal
x   <- as.integer(milk$clade)
nObs <- nrow(milk)
nVar <- max(x)
aMu <- 0.6         
aSD <- sigmaSD <- 10

dat <- list(nObs=nObs, nVar=nVar, obs=obs, x=x, aMu=aMu, aSD=aSD, 
  sigmaSD=sigmaSD)

intMod <- stan(file="12.interceptMod.stan", data=dat, iter=2000,
 chains=4, seed=867.5309)

round(summary(intMod, pars=c("alpha", "sigma"),  
  probs = c(0.025, 0.5, 0.975))$summary,2)
```
